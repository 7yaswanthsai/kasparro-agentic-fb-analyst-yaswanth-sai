# Kasparro â€” Agentic Facebook Performance Analyst  
Author: **Yaswanth Sai**

This project implements a multi-agent, production-style analysis pipeline for Facebook Ads.  
It diagnoses CTR/ROAS changes, validates hypotheses with evidence, and generates creative ideas grounded in the dataset.

This version includes all improvements requested in **P0, P1 and P2**:  
- Schema validation  
- Enhanced logging  
- Retry + backoff  
- Integration tests  
- Schema drift detection  
- Lightweight metrics layer  

---

## ğŸš€ Quick Start

```bash
python -V              # Python >= 3.10 recommended

python -m venv .venv
.venv\Scripts\activate     # Windows
# source .venv/bin/activate   # macOS / Linux

pip install -r requirements.txt

python src/run.py "Analyze ROAS drop"
````

Outputs will appear in:

```
reports/report.md
reports/insights.json
reports/creatives.json
logs/log_<timestamp>.json
```

---

# ğŸ“ Project Structure

```
kasparro-agentic-fb-analyst-yaswanth-sai/
â”œâ”€â”€ agent_graph.md
â”œâ”€â”€ Makefile
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ config/
â”‚   â””â”€â”€ config.yaml
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ sample_fb_ads.csv
â”‚   â”œâ”€â”€ synthetic_fb_ads_undergarments.csv
â”‚   â””â”€â”€ README.md
â”œâ”€â”€ prompts/
â”‚   â”œâ”€â”€ planner_prompt.md
â”‚   â”œâ”€â”€ insight_prompt.md
â”‚   â””â”€â”€ creative_prompt.md
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ run.py
â”‚   â”œâ”€â”€ orchestrator.py
â”‚   â”œâ”€â”€ utils.py
â”‚   â””â”€â”€ agents/
â”‚       â”œâ”€â”€ planner.py
â”‚       â”œâ”€â”€ data_agent.py
â”‚       â”œâ”€â”€ insight_agent.py
â”‚       â”œâ”€â”€ evaluator.py
â”‚       â””â”€â”€ creative_generator.py
â”œâ”€â”€ reports/
â”œâ”€â”€ logs/
â””â”€â”€ tests/
    â”œâ”€â”€ test_data_agent.py
    â”œâ”€â”€ test_evaluator.py
    â”œâ”€â”€ test_pipeline.py
    â”œâ”€â”€ test_integration.py
    â”œâ”€â”€ test_metrics_layer.py
    â””â”€â”€ test_schema_drift.py
```

---

# âš™ï¸ Configuration

`config/config.yaml`:

```yaml
random_seed: 42
confidence_min: 0.6

data_csv: "data/synthetic_fb_ads_undergarments.csv"

output_dir: "reports"
logs_dir: "logs"

report_file: "reports/report.md"
insights_file: "reports/insights.json"
creatives_file: "reports/creatives.json"

schema_drift_mode: "warn"    # fail | warn | off
sample_window_days: 30
```

---

# ğŸ§  Agent Architecture

## 1. Planner Agent

Creates a structured task list covering:

* Data loading
* Insight generation
* Validation
* Creative generation
* Report compilation

## 2. Data Agent

Production-style data layer:

* Schema validation
* Type enforcement
* Null-pattern checks
* Configurable schema drift detection (P2)
* Cleaned numeric columns
* Campaign + time-series summaries
* Detailed logs

## 3. Insight Agent

Generates data-backed hypotheses using:

* CTR/ROAS trends
* Spend and efficiency signals
* Message performance patterns
* Frequency / fatigue detection

## 4. Evaluator Agent

Validates hypotheses with:

* Baseline vs current comparisons
* Metric deltas
* Correlation strength
* Confidence scoring
* Structured decision explanations

## 5. Creative Generator

Produces data-grounded variations:

* Extracts themes from messages
* Recombines strong phrases
* Generates suggestions per low-CTR campaign

---

# ğŸ“„ CLI Example

```bash
python src/run.py "Analyze ROAS drop"
```

---

# ğŸ“Š Example Output

### `insights.json`

Contains: plan, summary, hypotheses, validated decisions.

### `creatives.json`

Campaign â†’ Suggested messages.

### `report.md`

Readable summary for marketers.

---

# ğŸ§ª Tests (P1)

Run:

```bash
pytest -q
```

Expected:

```
10 passed
```

Tests cover:

* Schema validation
* Schema drift behaviour
* Evaluator scoring
* Metrics layer
* Retry wrapping
* Full pipeline integration

---

# ğŸ” Observability & Logging (P0 + P1)

Logs contain:

* Step timings
* Hypothesis count
* Evaluator decisions
* Creative summaries
* Retry attempts
* Schema drift warnings/errors
* Metrics snapshot

A new log file is written for every run.

---

# ğŸ“Š Metrics Layer (P2)

Lightweight, in-memory:

* Counters (rows, hypotheses, valid insights)
* Timers (data_load, evaluation, creative_generation, run.total)

Included directly in the final log.

---

# ğŸ” Reproducibility

* Deterministic (seeded)
* Pinned dependencies
* Strict schema governance
* Fully configurable thresholds

---

# ğŸ·ï¸ Release Instructions

```bash
git add .
git commit -m "P0 P1 P2 improvements: schema validation, drift detection, retry logic, metrics, tests"
git push
```

Create a PR titled **â€œself-reviewâ€** summarizing:

* Architecture decisions
* Why the approach was chosen
* Known limitations
* Possible next steps

---

# ğŸ¯ Submission Summary (For Reviewer)

### **P0 â€“ Completed**

âœ” Schema validation (required columns, types, null-patterns)

âœ” Fail-fast behaviour

âœ” Enhanced logging with step timings and structured events

### **P1 â€“ Completed**

âœ” Retry logic with exponential backoff

âœ” Integration tests for full pipeline

âœ” Edge-case tests for DataAgent and Evaluator

### **P2 â€“ Completed**

âœ” Configurable schema drift detector (fail/warn/off)

âœ” Lightweight metrics layer with timer + counter snapshots

This repository is updated and ready for review.
